{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cross.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGe1qXoIyAyt"
      },
      "source": [
        "!pip install panda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRoyBCN6Yy7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d527232-5b40-4da7-9cfa-083738d5ca95"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 8.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 32.1MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 40.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=19698f544b15b3e203f6dfa29ad3ca6c1a55ae807504fb87b91461850325b9fb\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgM-fNFkyLMc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import torch\n",
        "import transformers as ppb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPitOV5qNntI"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM2HYLLAyXn8"
      },
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlmA2B8PB2jp"
      },
      "source": [
        "#df = pd.read_csv(\"de-en.test.en\", delimiter='\\t', header=None)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mACJIB3atLO",
        "outputId": "15d45707-7b1c-4d5b-8544-f90dcc50352d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nidoRPYwbe4O"
      },
      "source": [
        "# df=[]\n",
        "# df.append( pd.read_csv(\"/content/drive/My Drive/de-en/de-en.test.en\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))\n",
        "# df.append(pd.read_csv(\"/content/drive/My Drive/de-en/de-en.test.de\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))\n",
        "# df.append(pd.read_csv(\"/content/drive/My Drive/bucc2018-fr-en.test/bucc2018/fr-en/fr-en.test.en\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))\n",
        "# df.append(pd.read_csv(\"/content/drive/My Drive/bucc2018-fr-en.test/bucc2018/fr-en/fr-en.test.fr\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))\n",
        "# df.append(pd.read_csv(\"/content/drive/My Drive/bucc2018-ru-en.test/bucc2018/ru-en/ru-en.test.en\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))\n",
        "# df.append(pd.read_csv(\"/content/drive/My Drive/bucc2018-ru-en.test/bucc2018/ru-en/ru-en.test.ru\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))\n",
        "# df.append(pd.read_csv(\"/content/drive/My Drive/bucc2018-zh-en.test/bucc2018/zh-en/zh-en.test.en\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))\n",
        "# df.append(pd.read_csv(\"/content/drive/My Drive/bucc2018-zh-en.test/bucc2018/zh-en/zh-en.test.zh\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05ED0Qxzx-rw"
      },
      "source": [
        "df=[(pd.read_csv(\"/content/drive/My Drive/de-en/de-en.test.en\", delimiter='\\t', header=None,names=['encodingno', 'sentence'])),\n",
        "(pd.read_csv(\"/content/drive/My Drive/de-en/de-en.test.de\", delimiter='\\t', header=None,names=['encodingno', 'sentence'])),\n",
        "(pd.read_csv(\"/content/drive/My Drive/bucc2018-fr-en.test/bucc2018/fr-en/fr-en.test.en\", delimiter='\\t', header=None,names=['encodingno', 'sentence'])),\n",
        "(pd.read_csv(\"/content/drive/My Drive/bucc2018-fr-en.test/bucc2018/fr-en/fr-en.test.fr\", delimiter='\\t', header=None,names=['encodingno', 'sentence'])),\n",
        "(pd.read_csv(\"/content/drive/My Drive/bucc2018-ru-en.test/bucc2018/ru-en/ru-en.test.en\", delimiter='\\t', header=None,names=['encodingno', 'sentence'])),\n",
        "(pd.read_csv(\"/content/drive/My Drive/bucc2018-ru-en.test/bucc2018/ru-en/ru-en.test.ru\", delimiter='\\t', header=None,names=['encodingno', 'sentence'])),\n",
        "(pd.read_csv(\"/content/drive/My Drive/bucc2018-zh-en.test/bucc2018/zh-en/zh-en.test.en\", delimiter='\\t', header=None,names=['encodingno', 'sentence'])),\n",
        "(pd.read_csv(\"/content/drive/My Drive/bucc2018-zh-en.test/bucc2018/zh-en/zh-en.test.zh\", delimiter='\\t', header=None,names=['encodingno', 'sentence']))]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5MsRQeb_S9h"
      },
      "source": [
        "# for i in range(0,8):\n",
        "#   df[i]=df[i][:]\n",
        "  \n",
        "  # df2=df2[:10]\n",
        "  # df3 = df3[:10]\n",
        "  # df4=df4[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2U43L36LIkO",
        "outputId": "0834c155-5a43-4f23-bdff-c31595a6803f"
      },
      "source": [
        "print(df[0])\n",
        "print(df[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          encodingno                                           sentence\n",
            "0       en-000000001  Asgharzadeh became well known as a leader of t...\n",
            "1       en-000000002  Asgharzadeh also served as a military commande...\n",
            "2       en-000000003  Five of six respondents who served in the mili...\n",
            "3       en-000000004  After 1988 Asgharzadeh began calling for more ...\n",
            "4       en-000000005          I have been working hard on this project.\n",
            "...              ...                                                ...\n",
            "392380  en-000396530  Cassie also has a recurring role on the \"Tiny ...\n",
            "392381  en-000396531  However, when she returns, wearing a skirt wit...\n",
            "392382  en-000396532  In the comic tie-in to \"\", Cassie is with the ...\n",
            "392383  en-000396533  Her attire is similar to that of Wonder Woman'...\n",
            "392384  en-000396534  She and the other Titans try to help Conner st...\n",
            "\n",
            "[392385 rows x 2 columns]\n",
            "          encodingno                                           sentence\n",
            "0       de-000000001  Die zweiwertige aussagenlogische Semantik entw...\n",
            "1       de-000000002  Die Aussagensemantik und-axiomatik kombinierte...\n",
            "2       de-000000003  Die Weiterentwicklung der Aussagenlogik der St...\n",
            "3       de-000000004  Den ersten aussagenlogischen Kalkül mit Schlus...\n",
            "4       de-000000005  Er war die Vorlage für den Aussagenkalkül von ...\n",
            "...              ...                                                ...\n",
            "411974  de-000413880  Des Weiteren wurde das Petitionsrecht in der V...\n",
            "411975  de-000413881  Im Mai 2005 trat eine neue Verordnung in Kraft...\n",
            "411976  de-000413882  Dies betrifft vor Allem Opfer von Justizwillkü...\n",
            "411977  de-000413883  Berichten zufolge versuchen sie aus diesem Gru...\n",
            "411978  de-000413884  Arbeitern des Nationalen Petitionsbüros wird z...\n",
            "\n",
            "[411979 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bei9YQr_a9L"
      },
      "source": [
        "#batch_1[0].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZT26U-7A_og9"
      },
      "source": [
        "model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-multilingual-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb7us1DZ_wJ-"
      },
      "source": [
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csX0XjVFraSZ"
      },
      "source": [
        "#check for GPU access\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUlgvWD3sWlb"
      },
      "source": [
        "# print('Number of training sentences: {:,}\\n'.format(df[0].shape[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpnjI7GzLgtl"
      },
      "source": [
        "# sentences = ['Asgharzadeh became well known as a leader of the embassy takeover.', \n",
        "#  'Asgharzadeh also served as a military commander in the war with Iraq for six months.', \n",
        "#  'Five of six respondents who served in the military since the 1990’s were subject to abuse as soldiers.',\n",
        "#  'After 1988 Asgharzadeh began calling for more openness and \"voicing his opposition to the clerics\\'policies.\"',\n",
        "#  'In 1988 Asgharzadeh was elected to Parliament representing a district in Tehran.',\n",
        "#  'He was later arrested for publishing the reformist \"Salam\" newspaper which was critical of the government.',\n",
        "#  'Since the development of the telescope, the field of supernova discovery has expanded to other galaxies.',\n",
        "#   'The supernova explosion that formed the Vela Supernova Remnant most likely occurred 10,000–20,000 years ago.',\n",
        "#  'In 1976, NASA astronomers suggested that inhabitants of the southern hemisphere may have witnessed this explosion and recorded it symbolically.',\n",
        "#  'A year later, archaeologist George Michanowsky recalled some incomprehensible ancient markings in Bolivia that were left by Native Americans.']\n",
        "# encodingno= ['en-000000001', 'en-000000002', 'en-000000003', 'en-000000004',\n",
        "#  'en-000000005', 'en-000000006', 'en-000000007', 'en-000000008',\n",
        "#  'en-000000009', 'en-000000010']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7vc_jrosgk_"
      },
      "source": [
        "sentences = [df[0].sentence.values,df[1].sentence.values] #,df[2].sentence.values,df[3].sentence.values,df[4].sentence.values,df[5].sentence.values,df[6].sentence.values,df[7].sentence.values]\n",
        "#encodingno=[df[0].sentence.values,df[1].sentence.values,df[2].sentence.values,df[3].sentence.values,df[4].sentence.values,df[5].sentence.values,df[6].sentence.values,df[7].sentence.values]]\n",
        "# for i in range (0,9):\n",
        "#   sentences[ df[i].sentence.values)\n",
        "#   encodingno.append( df[0].encodingno.values)\n",
        " \n",
        "#print((sentences[0]))\n",
        "# print((sentences[1]))\n",
        "# print(sentences[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1psMELpUsqHI",
        "outputId": "29e4cec2-7952-46e4-902b-877ed67acc92"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF24hZzntRcL"
      },
      "source": [
        "#print('test', tokenizer.encode_plus(sentences[0]))\n",
        "# print(' Original: ', sentences[0])\n",
        "# # Print the sentence split into tokens.\n",
        "# print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
        "# print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqH4M2wX0hvz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586ed25a-89f2-4971-97b6-bed3ab5fe8c5"
      },
      "source": [
        "sentences[0]=sentences[0][:600]\n",
        "sentences[1]=sentences[1][:600]\n",
        "print(len(sentences[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM6atJMDMYwk"
      },
      "source": [
        "def getrep(tempsent):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "#for i in range(0,8):\n",
        "  for sent in tempsent:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 64,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt')# Return pytorch tensors.\n",
        "     \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # input_ids = torch.cat(input_ids, dim=0)\n",
        "    # attention_masks = torch.cat(attention_masks, dim=0)    \n",
        "\n",
        "  input_ids = torch.cat(input_ids, dim=0)\n",
        "#print(input_ids)\n",
        "#print(attention_masks)\n",
        "\n",
        "  attention_masks = torch.cat(attention_masks, dim=0) \n",
        "\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask = attention_masks)\n",
        "  vect=last_hidden_states[0][:,0,:]\n",
        "  return vect\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzkUNimfU62b",
        "outputId": "c9e3bb19-d259-4f78-8b8a-be491c6f4d59"
      },
      "source": [
        "vectenglish=getrep(sentences[0])\n",
        "vectde=getrep(sentences[1])\n",
        "# vectfr=getrep(sentences[3])\n",
        "# vectru=getrep(sentences[5])\n",
        "# vectzh=getrep(sentences[7])\n",
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "outputs=[]\n",
        "score=0\n",
        "\n",
        "#for j in range(0,15):\n",
        "for i in range(0,600):\n",
        "    output =  cos(vectenglish[4,:],vectde[i,:])\n",
        "    outputs.append(output)\n",
        "\n",
        "x=torch.tensor(outputs)\n",
        "  #print(x)\n",
        "a=torch.max(x)\n",
        "  #print(a)\n",
        "ind=(x==a).nonzero()\n",
        "#print(ind)\n",
        "\n",
        "print(\"english statement \"+\"\\n\")\n",
        "print(sentences[0][4])\n",
        "print(\"\\n\")\n",
        "print(\"translation\"+\"\\n\")\n",
        "print(sentences[1][ind])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "english statement \n",
            "\n",
            "I have been working hard on this project.\n",
            "\n",
            "\n",
            "translation\n",
            "\n",
            "Ich habe hart an diesem Projekt gearbeitet.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxG7vfD7Pv2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "147d26aa-0659-4cce-a6ba-fd927f769d49"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)\n",
        "outputs=[]\n",
        "score=0\n",
        "\n",
        "for j in range(0,600):\n",
        "  for i in range(0,600):\n",
        "    output =  cos(vectenglish[j,:],vectde[i,:])\n",
        "    outputs.append(output)\n",
        "\n",
        "  x=torch.tensor(outputs)\n",
        "  #print(x)\n",
        "  a=torch.max(x)\n",
        "  #print(a)\n",
        "  ind=(x==a).nonzero()\n",
        "#print(ind)\n",
        "\n",
        "  # print(\"english statement \"+\"\\n\")\n",
        "  # print(sentences[0][j])\n",
        "  # print(\"\\n\")\n",
        "  # print(\"translation\"+\"\\n\")\n",
        "  # print(sentences[1][ind])\n",
        "   \n",
        "  score += sentence_bleu(sentences[0][j], sentences[1][ind], weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  outputs=[]\n",
        "\n",
        "print(score/600)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6555968708012706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hai0X1Jtcx5V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d4b549e-d728-4be2-e6be-abbd860424da"
      },
      "source": [
        " from nltk.translate.bleu_score import sentence_bleu\n",
        " score = sentence_bleu(sentences[0][4], sentences[1][570], weights=(0.25, 0.25, 0.25, 0.25))\n",
        " print(score)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8043610129914494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHEQnqhjHsBK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX4a30Opd4zZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-1H9NwZmLAX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-s9ZrQzz_3rP"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38D_AC4hGg1K"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaO60yRgGhiI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMd9E8d5Gl9G"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RioJmc0EqhF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}