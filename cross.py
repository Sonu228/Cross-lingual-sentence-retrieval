# -*- coding: utf-8 -*-
"""Cross.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A5pDm8W-kLeElBI6aV6FFYQ93-mhzYpK
"""

!pip install panda

!pip install transformers

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
import transformers as ppb
import warnings
warnings.filterwarnings('ignore')

# from google.colab import files
# uploaded = files.upload()

#df = pd.read_csv("de-en.test.en", delimiter='\t', header=None)

from google.colab import drive
drive.mount('/content/drive')

# df=[]
# df.append( pd.read_csv("/content/drive/My Drive/de-en/de-en.test.en", delimiter='\t', header=None,names=['encodingno', 'sentence']))
# df.append(pd.read_csv("/content/drive/My Drive/de-en/de-en.test.de", delimiter='\t', header=None,names=['encodingno', 'sentence']))
# df.append(pd.read_csv("/content/drive/My Drive/bucc2018-fr-en.test/bucc2018/fr-en/fr-en.test.en", delimiter='\t', header=None,names=['encodingno', 'sentence']))
# df.append(pd.read_csv("/content/drive/My Drive/bucc2018-fr-en.test/bucc2018/fr-en/fr-en.test.fr", delimiter='\t', header=None,names=['encodingno', 'sentence']))
# df.append(pd.read_csv("/content/drive/My Drive/bucc2018-ru-en.test/bucc2018/ru-en/ru-en.test.en", delimiter='\t', header=None,names=['encodingno', 'sentence']))
# df.append(pd.read_csv("/content/drive/My Drive/bucc2018-ru-en.test/bucc2018/ru-en/ru-en.test.ru", delimiter='\t', header=None,names=['encodingno', 'sentence']))
# df.append(pd.read_csv("/content/drive/My Drive/bucc2018-zh-en.test/bucc2018/zh-en/zh-en.test.en", delimiter='\t', header=None,names=['encodingno', 'sentence']))
# df.append(pd.read_csv("/content/drive/My Drive/bucc2018-zh-en.test/bucc2018/zh-en/zh-en.test.zh", delimiter='\t', header=None,names=['encodingno', 'sentence']))

df=[(pd.read_csv("/content/drive/My Drive/de-en/de-en.test.en", delimiter='\t', header=None,names=['encodingno', 'sentence'])),
(pd.read_csv("/content/drive/My Drive/de-en/de-en.test.de", delimiter='\t', header=None,names=['encodingno', 'sentence'])),
(pd.read_csv("/content/drive/My Drive/bucc2018-fr-en.test/bucc2018/fr-en/fr-en.test.en", delimiter='\t', header=None,names=['encodingno', 'sentence'])),
(pd.read_csv("/content/drive/My Drive/bucc2018-fr-en.test/bucc2018/fr-en/fr-en.test.fr", delimiter='\t', header=None,names=['encodingno', 'sentence'])),
(pd.read_csv("/content/drive/My Drive/bucc2018-ru-en.test/bucc2018/ru-en/ru-en.test.en", delimiter='\t', header=None,names=['encodingno', 'sentence'])),
(pd.read_csv("/content/drive/My Drive/bucc2018-ru-en.test/bucc2018/ru-en/ru-en.test.ru", delimiter='\t', header=None,names=['encodingno', 'sentence'])),
(pd.read_csv("/content/drive/My Drive/bucc2018-zh-en.test/bucc2018/zh-en/zh-en.test.en", delimiter='\t', header=None,names=['encodingno', 'sentence'])),
(pd.read_csv("/content/drive/My Drive/bucc2018-zh-en.test/bucc2018/zh-en/zh-en.test.zh", delimiter='\t', header=None,names=['encodingno', 'sentence']))]

# for i in range(0,8):
#   df[i]=df[i][:]
  
  # df2=df2[:10]
  # df3 = df3[:10]
  # df4=df4[:10]

print(df[0])
print(df[1])

#batch_1[0].value_counts()

model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-multilingual-uncased')

# Load pretrained model/tokenizer
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

#check for GPU access
import torch

# print('Number of training sentences: {:,}\n'.format(df[0].shape[0]))

# sentences = ['Asgharzadeh became well known as a leader of the embassy takeover.', 
#  'Asgharzadeh also served as a military commander in the war with Iraq for six months.', 
#  'Five of six respondents who served in the military since the 1990’s were subject to abuse as soldiers.',
#  'After 1988 Asgharzadeh began calling for more openness and "voicing his opposition to the clerics\'policies."',
#  'In 1988 Asgharzadeh was elected to Parliament representing a district in Tehran.',
#  'He was later arrested for publishing the reformist "Salam" newspaper which was critical of the government.',
#  'Since the development of the telescope, the field of supernova discovery has expanded to other galaxies.',
#   'The supernova explosion that formed the Vela Supernova Remnant most likely occurred 10,000–20,000 years ago.',
#  'In 1976, NASA astronomers suggested that inhabitants of the southern hemisphere may have witnessed this explosion and recorded it symbolically.',
#  'A year later, archaeologist George Michanowsky recalled some incomprehensible ancient markings in Bolivia that were left by Native Americans.']
# encodingno= ['en-000000001', 'en-000000002', 'en-000000003', 'en-000000004',
#  'en-000000005', 'en-000000006', 'en-000000007', 'en-000000008',
#  'en-000000009', 'en-000000010']

sentences = [df[0].sentence.values,df[1].sentence.values] #,df[2].sentence.values,df[3].sentence.values,df[4].sentence.values,df[5].sentence.values,df[6].sentence.values,df[7].sentence.values]
#encodingno=[df[0].sentence.values,df[1].sentence.values,df[2].sentence.values,df[3].sentence.values,df[4].sentence.values,df[5].sentence.values,df[6].sentence.values,df[7].sentence.values]]
# for i in range (0,9):
#   sentences[ df[i].sentence.values)
#   encodingno.append( df[0].encodingno.values)
 
#print((sentences[0]))
# print((sentences[1]))
# print(sentences[2])

from transformers import BertTokenizer
print('Loading BERT tokenizer...')
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased', do_lower_case=True)

#print('test', tokenizer.encode_plus(sentences[0]))
# print(' Original: ', sentences[0])
# # Print the sentence split into tokens.
# print('Tokenized: ', tokenizer.tokenize(sentences[0]))
# print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))

sentences[0]=sentences[0][:600]
sentences[1]=sentences[1][:600]
print(len(sentences[0]))

def getrep(tempsent):
  input_ids = []
  attention_masks = []

#for i in range(0,8):
  for sent in tempsent:
    encoded_dict = tokenizer.encode_plus(
                        sent,                      # Sentence to encode.
                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'
                        max_length = 64,           # Pad & truncate all sentences.
                        pad_to_max_length = True,
                        return_attention_mask = True,   # Construct attn. masks.
                        return_tensors = 'pt')# Return pytorch tensors.
     
    input_ids.append(encoded_dict['input_ids'])
    
    # And its attention mask (simply differentiates padding from non-padding).
    attention_masks.append(encoded_dict['attention_mask'])

    # input_ids = torch.cat(input_ids, dim=0)
    # attention_masks = torch.cat(attention_masks, dim=0)    

  input_ids = torch.cat(input_ids, dim=0)
#print(input_ids)
#print(attention_masks)

  attention_masks = torch.cat(attention_masks, dim=0) 

  with torch.no_grad():
    last_hidden_states = model(input_ids, attention_mask = attention_masks)
  vect=last_hidden_states[0][:,0,:]
  return vect

vectenglish=getrep(sentences[0])
vectde=getrep(sentences[1])
# vectfr=getrep(sentences[3])
# vectru=getrep(sentences[5])
# vectzh=getrep(sentences[7])
cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)
outputs=[]
score=0

#for j in range(0,15):
for i in range(0,600):
    output =  cos(vectenglish[4,:],vectde[i,:])
    outputs.append(output)

x=torch.tensor(outputs)
  #print(x)
a=torch.max(x)
  #print(a)
ind=(x==a).nonzero()
#print(ind)

print("english statement "+"\n")
print(sentences[0][4])
print("\n")
print("translation"+"\n")
print(sentences[1][ind])

from nltk.translate.bleu_score import sentence_bleu

cos = torch.nn.CosineSimilarity(dim=0, eps=1e-6)
outputs=[]
score=0

for j in range(0,600):
  for i in range(0,600):
    output =  cos(vectenglish[j,:],vectde[i,:])
    outputs.append(output)

  x=torch.tensor(outputs)
  #print(x)
  a=torch.max(x)
  #print(a)
  ind=(x==a).nonzero()
#print(ind)

  # print("english statement "+"\n")
  # print(sentences[0][j])
  # print("\n")
  # print("translation"+"\n")
  # print(sentences[1][ind])
   
  score += sentence_bleu(sentences[0][j], sentences[1][ind], weights=(0.25, 0.25, 0.25, 0.25))
  outputs=[]

print(score/600)

from nltk.translate.bleu_score import sentence_bleu
 score = sentence_bleu(sentences[0][4], sentences[1][570], weights=(0.25, 0.25, 0.25, 0.25))
 print(score)











"""# New Section"""

